{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> <span style=\"color:firebrick\"> <font size=\"5\"> <b> USC Marshall School of Business </b> </font> </p> </span> \n",
    "\n",
    "<p style=\"text-align: center;\"> <b> <font font size=\"5\"> DSO 560 - Final Project </p> </b></font>\n",
    "\n",
    "<p style=\"text-align: center;\"> <b> Spring 2021 </b> </p>\n",
    "\n",
    "## <span style=\"color:black\"> <font size=\"3\">Group Maroon: Andrew Chuang, Janicia Chang, Ningchuan Peng, Zijing Wu</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:05:02.238809Z",
     "start_time": "2021-05-11T09:05:02.235808Z"
    }
   },
   "outputs": [],
   "source": [
    "# import relavent libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Classification\n",
    "\n",
    "### Executive Summary\n",
    "For this question, we build a classification model to predict the brands that the products belong to. \n",
    "- We only use the top 30 brands and labeled other brands as \"Other\".\n",
    "- We utilize all of the columns that contain relevant text for prediction.\n",
    "- Basic preprocessing: Lower case, Stopwords removal, Lemmatization with part of speech tagging.\n",
    "- Two different vectorization techniques are tried: Document Embeddings and TF-IDF Vectorization.\n",
    "- XGBoost classifier is used for modeling and 5-fold cross-validation is used to evaluate different vectorization methods.\n",
    "- TF-IDF Vectorization technique achieves higher accuracy and is chosen as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:22.189048Z",
     "start_time": "2021-05-11T09:04:12.807962Z"
    }
   },
   "outputs": [],
   "source": [
    "# read datasets\n",
    "product_df = pd.read_excel('Behold+product+data+04262021.xlsx', encoding = 'latin1') \n",
    "additional_tags = pd.read_csv('usc_additional_tags USC.csv', encoding = 'latin1')\n",
    "outfit_df = pd.read_csv('outfit_combinations USC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:22.313078Z",
     "start_time": "2021-05-11T09:04:22.190048Z"
    }
   },
   "outputs": [],
   "source": [
    "# group additional tags by product_id\n",
    "tags = additional_tags.groupby(['product_id']).agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:22.563134Z",
     "start_time": "2021-05-11T09:04:22.314078Z"
    }
   },
   "outputs": [],
   "source": [
    "# join additional tags with products dataset using product_id\n",
    "df = pd.merge(product_df, tags, on = 'product_id', how = 'left')\n",
    "df = df.fillna('Unknown').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:22.626293Z",
     "start_time": "2021-05-11T09:04:22.564134Z"
    }
   },
   "outputs": [],
   "source": [
    "# filterout the top 30 brands and label other brands as 'Other'\n",
    "top_30 = df['brand'].value_counts().nlargest(30).keys()\n",
    "df['brand_top30'] = df['brand'].apply(lambda x: x if x in top_30 else 'Other')\n",
    "df['product_active'] = df['product_active'].apply(lambda x: 1 if True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:25.252001Z",
     "start_time": "2021-05-11T09:04:22.627292Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a new column that contains all relavent text\n",
    "df['text'] = df[['brand_category', 'name', 'details', 'description', 'attribute_value']].agg(' '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:25.393033Z",
     "start_time": "2021-05-11T09:04:25.253002Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert all text into lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:25.408051Z",
     "start_time": "2021-05-11T09:04:25.394033Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to lemmatize sentences with Part of Speech tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:04:25.423056Z",
     "start_time": "2021-05-11T09:04:25.409052Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a funtion to remove stopwords and lemmatize sentence\n",
    "def text_cleaning(x):\n",
    "    words = x.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in stopwords.words('english') + ['unknown']:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_text = \" \".join(new_words)\n",
    "    lemmatized_text = lemmatize_sentence(cleaned_text)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:29:40.099278Z",
     "start_time": "2021-05-11T09:05:07.442994Z"
    }
   },
   "outputs": [],
   "source": [
    "# creat a new column after text cleaning \n",
    "df['cleaned_text'] = df['text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:34:37.711760Z",
     "start_time": "2021-05-11T09:34:37.697756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ruffle market dress loopy pink sistine tomato mid-length dress ruffle adjustable strap . bias cut . side seam invisible zipper make new york model wear size small 100 % rise sylk rise sylk organic cellulose fiber make natural waste rise bush stem .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:43:55.983977Z",
     "start_time": "2021-05-11T09:35:01.272628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a pre-trained word embeddings in spacy and apply to the dataset\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = df['cleaned_text'].astype(str).apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T09:44:08.126709Z",
     "start_time": "2021-05-11T09:43:55.984977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61355, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the document embeddings \n",
    "# each document is represented as a vector\n",
    "emb_array = np.array(list(doc.apply(lambda x: list(x.vector))))\n",
    "embeddings = pd.DataFrame(emb_array, columns = range(300))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:22:20.465503Z",
     "start_time": "2021-05-11T09:44:08.128701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Embeddings: \n",
      "\n",
      "Classification error of 5-folds:  [0.06372749 0.06299405 0.06364599 0.06494988 0.06160867]\n",
      "Mean classification error: 0.06338521717871404\n"
     ]
    }
   ],
   "source": [
    "# use 5-fold CV to evaluate model performance\n",
    "kfolds_classification = StratifiedKFold(n_splits = 5, random_state = 0, shuffle = True) \n",
    "# fit a xgboost classification model with document embeddings as x and brand names as y\n",
    "xgb_classification = xgb.XGBClassifier(eval_metric = 'merror')\n",
    "xgb_accuracy_cv = cross_val_score(xgb_classification, embeddings, df['brand_top30'], cv = kfolds_classification)\n",
    "print(\"Document Embeddings: \\n\")\n",
    "print(\"Classification error of 5-folds: \",1-xgb_accuracy_cv)\n",
    "print(\"Mean classification error:\",1-np.mean(xgb_accuracy_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:22:25.272371Z",
     "start_time": "2021-05-11T10:22:20.466504Z"
    }
   },
   "outputs": [],
   "source": [
    "# using tf-idf vectorizer to vectorize the dataset\n",
    "idf_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 max_features=1000,\n",
    "                                 min_df=5)\n",
    "tfidf = idf_vectorizer.fit_transform(df['cleaned_text'].astype(str))\n",
    "y = df['brand_top30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:29:53.154822Z",
     "start_time": "2021-05-11T10:22:25.273372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization: \n",
      "\n",
      "Classification error of 5-folds:  [0.05574118 0.05696357 0.05631163 0.05834895 0.05345938]\n",
      "Mean classification error: 0.05616494173254016\n"
     ]
    }
   ],
   "source": [
    "# fit a xgboost classification model with tfidf scores as x and brand names as y\n",
    "xgb_classification = xgb.XGBClassifier(eval_metric = 'merror')\n",
    "xgb_accuracy_cv = cross_val_score(xgb_classification, tfidf, y, cv = kfolds_classification)\n",
    "print(\"TF-IDF Vectorization: \\n\")\n",
    "print(\"Classification error of 5-folds: \",1-xgb_accuracy_cv)\n",
    "print(\"Mean classification error:\",1-np.mean(xgb_accuracy_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Reflection\n",
    "- In general, the performance of the two models are very close and beat our expectations. We were able to achieve a mean classfication error of 5.6% with an xgboost classification model without any hyperparameters adjustment.\n",
    "- If we have more time and computating power in the future, we will try deep learning models such as RNN and LSTM to see if we can further improve our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Recommendation\n",
    "\n",
    "### Executive Summary\n",
    "To solve the problem of recommending an outfit to a customer, our group has come up with an algorithm:\n",
    "- We use Regex to classify the products into five categories: Top, Bottom, One piece, Shoes, and Accessory. \n",
    "- We get document embeddings of the query. \n",
    "- We calculate the cosine distance between the query and each product and find the product with the lowest cosine distance.\n",
    "- If the product is in the expert defined outfit dataset, we will use the outfit combination directly.\n",
    "- If the product is not in the outfit dataset, we will use the product with the lowest cosine distance in each category to form an outfit combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:29:53.170826Z",
     "start_time": "2021-05-11T10:29:53.155822Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a funtion to replace words\n",
    "def replace(title, word, reword):\n",
    "    count = 0\n",
    "    if isinstance(title, str):\n",
    "        title = re.sub(word, reword, title, flags=re.IGNORECASE)\n",
    "    return title\n",
    "\n",
    "# create a function to find the category\n",
    "def define_cate(title, word):\n",
    "    count = 'None'\n",
    "    if isinstance(title, str):\n",
    "        count = re.findall(word, title, re.IGNORECASE)\n",
    "        if len(count) == 0:\n",
    "            count = 'None'\n",
    "        else:\n",
    "            count = count[-1]\n",
    "    return count.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:29:54.049024Z",
     "start_time": "2021-05-11T10:29:53.171826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None         30598\n",
       "Bottom       12279\n",
       "Top          11985\n",
       "Shoe          2851\n",
       "One piece     2094\n",
       "Accessory     1548\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create words for replacement\n",
    "top_replace = r'\\b(top|shirt|t-shirt|hoodie|shirt|jacket|sweatshirt|coat|sweater|tee)\\b'\n",
    "bot_replace = r'\\b(bottom|shorts|dress|skirt|pants?|trousers?|jeans?|leg)\\b'\n",
    "one_replace = r'\\b(one piece|gown|swimsuit|overcoat|blouse|parka|blazer|jumpsuit|romper)\\b'\n",
    "shoe_replace = r'\\b(shoes?|heels?|flip flops?|boots?|sneakers?|sandals?|loafers?|flats?|mules?)\\b'\n",
    "acce_replace = r'\\b(handbag|bag|backpack|purse|tote|hat)\\b'\n",
    "\n",
    "# replace the words in different categories\n",
    "df['name_cleaned'] = df['name'].apply(lambda x: x.lower())\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, top_replace, r'TOP'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, bot_replace, r'Bottom'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, one_replace, r'One Piece'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, shoe_replace, r'Shoe'))\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x: replace(x, acce_replace, r'Accessory'))\n",
    "\n",
    "# classify the products to 5 big categories\n",
    "big_category = r'\\bTop|Bottom|One Piece|Shoe|Accessory\\b'\n",
    "df['product_category'] = df['name_cleaned'].apply(lambda x: define_cate(x, big_category))\n",
    "df['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above value counts we can see that there are still about 30000 products without a defined category. This is because we only used the name cloumn to do our regex matching. Since we will use product categories for our recommendations, we cannot afford miscategorizing a top as a bottom. Hence, we made the decision to optimize for precision and sacrafice recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:29:54.065028Z",
     "start_time": "2021-05-11T10:29:54.051025Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a function for giving back the outfit \n",
    "def give_back_result(product_id, cos_dis):\n",
    "    \n",
    "    # if the product is in the outfit combination dataset\n",
    "    if product_id in list(outfit_df['product_id']):\n",
    "        print('We find an outfit.')\n",
    "        \n",
    "        # get the best outfit combination\n",
    "        outfit_id = list(outfit_df[outfit_df['product_id'] == product_id]['outfit_id'])[0]\n",
    "        final_outfit = outfit_df[outfit_df['outfit_id'] == outfit_id]\n",
    "        \n",
    "        # print out the final result\n",
    "        for i in range(final_outfit.shape[0]):\n",
    "            out_type = final_outfit.iloc[i, 2].capitalize()\n",
    "            out_brand = final_outfit.iloc[i, 3]\n",
    "            out_prod = final_outfit.iloc[i, 4]\n",
    "            out_id = final_outfit.iloc[i, 1]\n",
    "            print(f'{out_type}: {out_brand} {out_prod} ({out_id})')\n",
    "    \n",
    "    # if the product is not in the outfit combination dataset\n",
    "    else:\n",
    "        print(\"We use products combination instead of an outfit.\")\n",
    "        \n",
    "        # get the products with the lowest cosine distance in 5 categories\n",
    "        df['cos_dis'] = cos_dis.values()\n",
    "        big_cate_list = ['Top', 'Bottom', 'One piece', 'Shoe', 'Accessory']\n",
    "        best_comb = {}\n",
    "        for i in range(len(big_cate_list)):\n",
    "            cate = df[df['product_category']== big_cate_list[i]].\\\n",
    "                   sort_index().sort_values('cos_dis', kind='mergesort').reset_index(drop=True)        \n",
    "            best_comb[cate.loc[0, 'product_category']] = [cate.loc[0, 'cos_dis'], \n",
    "                                                          cate.loc[0, 'brand'],\n",
    "                                                          cate.loc[0, 'name'],\n",
    "                                                          cate.loc[0, 'product_id']]\n",
    "            \n",
    "        # choose one piece or top and bottom based on the avearge cosine distance\n",
    "        if best_comb['One piece'][0] > (best_comb['Top'][0]+best_comb['Bottom'][0])/2:\n",
    "            del best_comb[\"One piece\"]\n",
    "        else:\n",
    "            del best_comb[\"Top\"]\n",
    "            del best_comb[\"Bottom\"]\n",
    "        \n",
    "        # print out the final result\n",
    "        for key in best_comb.keys():\n",
    "            print(f'{key}: {best_comb[key][1]} {best_comb[key][2]} ({best_comb[key][3]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:29:54.081031Z",
     "start_time": "2021-05-11T10:29:54.066028Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a function for searching for products\n",
    "def search(query):\n",
    "    \n",
    "    # get document embeddings of the query\n",
    "    query_nlp = nlp(query).vector\n",
    "    \n",
    "    # calculate the cosine distance between query and other products\n",
    "    cos_dis = {}\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        cos_dis[i] = cosine(embeddings.loc[i,], query_nlp) \n",
    "    \n",
    "    # find the product with the lowest cosine distance\n",
    "    product_id = df.iloc[min(cos_dis, key=cos_dis.get),0]\n",
    "    \n",
    "    # run the function for giving back the outfit \n",
    "    give_back_result(product_id, cos_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:33:43.715063Z",
     "start_time": "2021-05-11T10:29:54.082031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your search query: \n",
      "Example: slim fitting, straight leg pant with a center back zipper and slightly cropped leg\n",
      "\n",
      "We use products combination instead of an outfit.\n",
      "One piece: A.L.C. Joana Jumpsuit (01EDYCA0E5S1N651MTXRA7AX60)\n",
      "Shoe: Citizens of Humanity Emannuelle Slim Boot (01EB2DRYX1B0QGMRBC1D3GS1C9)\n",
      "Accessory: A.L.C. Sadie Handbag (01EDYE843YM31CHVY8ED79DQYW)\n"
     ]
    }
   ],
   "source": [
    "# run the final test, please input nothing for example\n",
    "test_query = input('Please input your search query: ')\n",
    "if test_query == '':\n",
    "    test_query = 'slim fitting, straight leg pant with a center back zipper and slightly cropped leg'\n",
    "    print('Example: slim fitting, straight leg pant with a center back zipper and slightly cropped leg')\n",
    "    print()\n",
    "search(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the query 'slim fitting, straight leg pant with a center back zipper and slightly cropped leg'. The product that has the lowest cosine distance with this query using document embeddings is not in the outfit combinations dataset. Thus, we get the products with the lowest cosine distance in each category. Since One piece has a lower average cosine distance than Top and Bottom, the function finally gives us a product combination of One piece, Shoes, and Accessory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
